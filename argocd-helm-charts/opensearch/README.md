# Opensearch Cluster

## Example config

```yaml
opensearch:
  opensearchJavaOpts: "-Xms4g -Xmx4g"
  replicas: 3

  rbac:
    create: true

  # https://github.com/opensearch-project/helm-charts/pull/69/files
  majorVersion: 7

  resources:
    limits:
      memory: 6Gi
    requests:
      cpu: 200m
      memory: 4Gi

  persistence:
    enabled: true
    labels:
      enabled: false
    accessModes:
      - ReadWriteOnce
    size: 500Gi
    annotations: {}

  config:
    opensearch.yml: |
      cluster.name: opensearch-cluster
      # Bind to all interfaces because we don't know what IP address Docker will assign to us.
      network.host: 0.0.0.0
      # # minimum_master_nodes need to be explicitly set when bound on a public IP
      # # set to 1 to allow single node clusters
      # discovery.zen.minimum_master_nodes: 1
      # Setting network.host to a non-loopback address enables the annoying bootstrap checks. "Single-node" mode disables them again.
      # discovery.type: single-node
      # Start OpenSearch Security Demo Configuration
      # WARNING: revise all the lines below before you go into production
      indices.query.bool.max_clause_count: 4096

      # Report as elasticsearch 7.10 - for graylog not to complain (until v4.3 of graylog with opensearch support is released)
      compatibility.override_main_response_version: true

      # https://archivedocs.graylog.org/en/2.4/pages/faq.html#how-do-i-fix-the-deflector-exists-as-an-index-and-is-not-an-alias-error-message
      action.auto_create_index: false
      plugins:
        security:
          ssl:
            transport:
              pemcert_filepath: esnode.pem
              pemkey_filepath: esnode-key.pem
              pemtrustedcas_filepath: root-ca.pem
              enforce_hostname_verification: false
            http:
              enabled: false
              pemcert_filepath: esnode.pem
              pemkey_filepath: esnode-key.pem
              pemtrustedcas_filepath: root-ca.pem
          allow_unsafe_democertificates: true
          allow_default_init_securityindex: true
          authcz:
            admin_dn:
              - CN=kirk,OU=client,O=client,L=test,C=de
          enable_snapshot_restore_privilege: true
          check_snapshot_restore_write_privileges: true
          restapi:
            roles_enabled: ["all_access", "security_rest_api_access"]
          system_indices:
            enabled: true
            indices:
              [
                ".opendistro-alerting-config",
                ".opendistro-alerting-alert*",
                ".opendistro-anomaly-results*",
                ".opendistro-anomaly-detector*",
                ".opendistro-anomaly-checkpoints",
                ".opendistro-anomaly-detection-state",
                ".opendistro-reports-*",
                ".opendistro-notifications-*",
                ".opendistro-notebooks",
                ".opendistro-asynchronous-search-response*",
              ]

```

## How to increase the ES index limit

### Sample issue which you see in graylog logs

```raw
[194]: index [graylog_2], type [_doc], id [938e8b50-2d32-11ec-b77c-0ad5e9494873], message [ElasticsearchException[Elasticsearch exception [type=illegal_argument_exception, reason=Limit of total fields [1000] has been exceeded]]]
```

## Solution

The issue is related to elasticseach settings and those are stored internally (in a settings index/db) and
can only be adjusted via ES API request.

* Login to the opensearch `opensearch-cluster-master-0` pod

* Verify the existing setting of the index for which it is complaining. You can do so by running

```sh
curl -u $username:$password -XGET http://localhost:9200/$index_name/_settings?pretty=true
```

* Increase the limit by running

```sh
curl -u $username:$password -X PUT "http://localhost:9200/$index_num/_settings?pretty" -H 'Content-Type: application/json' -d'
 {
   "index" : {
     "mapping.total_fields.limit" : 2000
   }
 }'
```

NOTE: We should notice that this increases memory usage and differing field names SHOULD be avoided when possible.
But with graylog, fields are generated by graylog - so not something we can do much about.

## Setting users and passwords

get a shell inside opensearch master pod, and run:

```bash
cd plugins/opensearch-security/tools
chmod +x hash.sh
./hash.sh NEWPASSWORD
# use the generate hash from above - when modifying internal_users.yml to suit your needs
vi /usr/share/opensearch/plugins/opensearch-security/securityconfig/internal_users.yml
../securityadmin.sh -cd ../securityconfig/ -icl -nhnv -cacert ../../../config/root-ca.pem -cert ../../../config/kirk.pem -key ../../../config/kirk-key.pem
```

## snapshot/restore elk stack to s3

Create a secret with template functionality

* Create a secret and save it in a file

```bash
# kubectl create secret generic s3-backup -n graylog --dry-run=client --from-literal=username=admin --from-literal=password=xxxx -o yaml  | kubeseal --controller-namespace system --controller-name sealed-secrets -o yaml - > s3-backup.yaml
```

* Edit the above file add the below config under spec.template.data and save it

```yaml
      config.yml: |
        client:
          hosts: opensearch-cluster-master
          username: {{ index . "username" }}
          password: {{ index . "password" }}
          port: 9200
          timeout: 10
        logging:
          blacklist: ["elasticsearch", "urllib3"]
          logformat: default
          logfile:
          loglevel: INFO
```

## Down sizing the cluster

### Down sizing might take hours, so its not a 30 min job. so relax and enjoy :)

* Stop routing any shards to the node you want.

  NOTE: Always take the last node in the cluster. if you are downsizing.
  f.exp, if you cluster has 12 pods and start with 12th pod and put the IP in the curl command given below.

```sh
# curl -v -H 'Content-type: application/json' -XPUT 'http://admin:lolpassword@opensearch-cluster-master:9200/_cluster/settings' -d '{
  "transient" :{
     "cluster.routing.allocation.exclude._ip" : "<last-pod-in-the-cluster-ip>"
   }
}'
```

* Verify if all the shards are moved

```bash
# curl -s http://admin:lolpassword@opensearch-cluster-master:9200/_cat/shards | grep <last-pod-name-in-the-cluster>

# `relocating_shards` should be **0**

# curl -s http://admin:lolpassworl@opensearch-cluster-master:9200/_cluster/health | jq
{
  "cluster_name": "opensearch-cluster",
  "status": "green",
  "timed_out": false,
  "number_of_nodes": 12,
  "number_of_data_nodes": 12,
  "discovered_master": true,
  "active_primary_shards": 3963,
  "active_shards": 3979,
  "relocating_shards": 0,
  "initializing_shards": 0,
  "unassigned_shards": 0,
  "delayed_unassigned_shards": 0,
  "number_of_pending_tasks": 0,
  "number_of_in_flight_fetch": 0,
  "task_max_waiting_in_queue_millis": 0,
  "active_shards_percent_as_number": 100
}
```

* Set the replicas count to n - 1 (12 pods in the cluster - 1 == 11)
  NOTE: Do this only when cluster is green

```yaml
opensearch:
  replicas: 11
```

* Sync it on the argocd and it will restart the whole cluster and remove the last pod in the cluster.
