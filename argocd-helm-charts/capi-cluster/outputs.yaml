---
# Source: capi-cluster/charts/aws/templates/AWSCluster.yaml
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSCluster
metadata:
  name: kubeaid-demo-aws
  labels:
    cluster.x-k8s.io/name: kubeaid-demo-aws
spec:
  region: us-east-1
  sshKeyName: kubeaid-demo
  bastion: 
    enabled: true
  network:
    vpc:
      cidrBlock: 10.14.0.0/22
      availabilityZoneSelection: Ordered
      availabilityZoneUsageLimit: 3
    cni:
      cniIngressRules:
        - description: Cilium
          protocol: "tcp"
          fromPort: 4240
          toPort: 4240
        - description: Cilium VXLAN overlay
          protocol: "udp"
          fromPort: 8472
          toPort: 8472
        - description: Cilium health checks
          protocol: "icmp"
          fromPort: 8
          toPort: 8
  controlPlaneLoadBalancer:
    loadBalancerType: nlb
    scheme: internet-facing
---
# Source: capi-cluster/charts/aws/templates/AWSMachineTemplate.yaml
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSMachineTemplate
metadata:
  name: kubeaid-demo-aws-control-plane
spec:
  template:
    spec:
      iamInstanceProfile: control-plane.cluster-api-provider-aws.sigs.k8s.io
      # Required by CAPA (Cluster API Provider AWS), when using IAM Roles instead of credentials
      # stored in Kubernetes Secret.
      # REFER : https://github.com/kubernetes-sigs/cluster-api-provider-aws/issues/4247.
      instanceMetadataOptions:
        httpEndpoint: enabled
        httpPutResponseHopLimit: 2
        httpTokens: optional # Both IMDSv1 and IMDSv2 are enabled.
        instanceMetadataTags: disabled
      instanceType: t4g.medium
      sshKeyName: 
      imageLookupBaseOS: ubuntu-20.04
      publicIP: 
      ami: 
        id: ami-0c4219b11327ef260
      rootVolume:
        size: 35
---
# Source: capi-cluster/charts/aws/templates/AWSMachineTemplate.yaml
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSMachineTemplate
metadata:
  name: kubeaid-demo-aws-primary
spec:
  template:
    spec:
      ami: 
        id: ami-0c4219b11327ef260
      iamInstanceProfile: nodes.cluster-api-provider-aws.sigs.k8s.io
      instanceMetadataOptions:
        httpEndpoint: enabled
        httpPutResponseHopLimit: 2
        httpTokens: optional
        instanceMetadataTags: disabled
      instanceType: t4g.medium
      sshKeyName: kubeaid-demo
      imageLookupBaseOS: ubuntu-20.04
      rootVolume:
        size: 35
---
# Source: capi-cluster/charts/aws/templates/AWSMachineTemplate.yaml
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSMachineTemplate
metadata:
  name: kubeaid-demo-aws-secondary
spec:
  template:
    spec:
      ami: 
        id: ami-0c4219b11327ef260
      iamInstanceProfile: nodes.cluster-api-provider-aws.sigs.k8s.io
      instanceMetadataOptions:
        httpEndpoint: enabled
        httpPutResponseHopLimit: 2
        httpTokens: optional
        instanceMetadataTags: disabled
      instanceType: t4g.medium
      sshKeyName: kubeaid-demo
      imageLookupBaseOS: ubuntu-20.04
      rootVolume:
        size: 35
---
# Source: capi-cluster/charts/aws/templates/Cluster.yaml
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: kubeaid-demo-aws
  labels:
    cluster.x-k8s.io/name: kubeaid-demo-aws
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 10.244.0.0/16
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: KubeadmControlPlane
    name: kubeaid-demo-aws-control-plane
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
    kind: AWSCluster
    name: kubeaid-demo-aws
---
# Source: capi-cluster/templates/provider-aws.yaml
apiVersion: operator.cluster.x-k8s.io/v1alpha2
kind: InfrastructureProvider
metadata:
  name: aws
  namespace: capi-cluster
spec:
  configSecret:
    name: cloud-credentials
    namespace: capi-cluster
  version: v2.7.1
  fetchConfig:
    url: https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases/v2.7.1/infrastructure-components.yaml
  deployment:
    nodeSelector:
      node-role.kubernetes.io/control-plane: ""
    tolerations:
      - key: "node-role.kubernetes.io/control-plane"
        operator: "Exists"
        effect: "NoSchedule"
  manager:
    cacheNamespace: capi-cluster
    metrics:
      bindAddress: ":8181"
    syncPeriod: 10m0s
    featureGates:
      MachinePool: false
---
# Source: capi-cluster/charts/aws/templates/KubeadmConfigTemplate.yaml
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: kubeaid-demo-aws-primary
spec:
  template:
    spec:
      joinConfiguration:
        nodeRegistration:
          kubeletExtraArgs:
            cloud-provider: external
          name: '{{ ds.meta_data.local_hostname }}'
      users:
        - name: archi
          sshAuthorizedKeys:
            - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCMB0BJ1vL53/xiDCfswJWOrBYc39MbnmMnAK9ATohDqgT1ZrvLODbw5T1NTOLsVyuWNXMB9HfPPXg3Qm5L5UXoIj5JeyIY1ArUxjdW5LI4fsYgYHfRSv3gSZFpuJj12+myBxdoNYCDNDrzYF5T3D2is0lqcSG+3UerkY7ox8WuHMSO69bUAn72ugw01TgycdXpX5jvGLb84yl2Xct1/YFsh9UKhVYWx8qdz5r7FM4h5KDIIi2yjAEjwm4DfYKT/9dlzTodzeim4Z755AGXpFWjlQYaV8vNLHhIkDEtDOZTDtePFwQ64cw1e8cYzuRqDRxCVA9mWQtWwbMgklAv5h0ZTu8cJ9XcomQV9W2NCieBJyu+bj71AKFg3qTD9aMBq2zBXshoZwvmPlrIfAreALPEcPQvn6T8vHGvfk6Pf91B7I1Tu9ds0X0Vp5KELR6wwZQx2JKwMJillzC4Z3GY4/oi5J5C6WnlB3TxCvsivBo3TtEc3phylnfHNvRbwCqch2k= robot-webservice-app-user
---
# Source: capi-cluster/charts/aws/templates/KubeadmConfigTemplate.yaml
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: kubeaid-demo-aws-secondary
spec:
  template:
    spec:
      joinConfiguration:
        nodeRegistration:
          kubeletExtraArgs:
            cloud-provider: external
          name: '{{ ds.meta_data.local_hostname }}'
      users:
        - name: archi
          sshAuthorizedKeys:
            - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCMB0BJ1vL53/xiDCfswJWOrBYc39MbnmMnAK9ATohDqgT1ZrvLODbw5T1NTOLsVyuWNXMB9HfPPXg3Qm5L5UXoIj5JeyIY1ArUxjdW5LI4fsYgYHfRSv3gSZFpuJj12+myBxdoNYCDNDrzYF5T3D2is0lqcSG+3UerkY7ox8WuHMSO69bUAn72ugw01TgycdXpX5jvGLb84yl2Xct1/YFsh9UKhVYWx8qdz5r7FM4h5KDIIi2yjAEjwm4DfYKT/9dlzTodzeim4Z755AGXpFWjlQYaV8vNLHhIkDEtDOZTDtePFwQ64cw1e8cYzuRqDRxCVA9mWQtWwbMgklAv5h0ZTu8cJ9XcomQV9W2NCieBJyu+bj71AKFg3qTD9aMBq2zBXshoZwvmPlrIfAreALPEcPQvn6T8vHGvfk6Pf91B7I1Tu9ds0X0Vp5KELR6wwZQx2JKwMJillzC4Z3GY4/oi5J5C6WnlB3TxCvsivBo3TtEc3phylnfHNvRbwCqch2k= robot-webservice-app-user
---
# Source: capi-cluster/charts/aws/templates/KubeadmControlPlane.yaml
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: KubeadmControlPlane
metadata:
  name: kubeaid-demo-aws-control-plane
  annotations:
    controlplane.cluster.x-k8s.io/skip-kube-proxy: ""
spec:
  kubeadmConfigSpec:
    clusterConfiguration:
      apiServer:
        extraArgs:
          cloud-provider: external
          audit-log-maxage: "10"
          audit-log-maxbackup: "1"
          audit-log-maxsize: "100"
          audit-log-path: /var/log/kube-apiserver-audit.logs
          audit-policy-file: /srv/kubernetes/audit.yaml
        extraVolumes:
          - hostPath: /srv/kubernetes/audit.yaml
            mountPath: /srv/kubernetes/audit.yaml
            name: audit-policy-file
            pathType: FileOrCreate
            readOnly: true
          - hostPath: /var/log/kube-apiserver-audit.logs
            mountPath: /var/log/kube-apiserver-audit.logs
            name: log-backend
            pathType: FileOrCreate
      etcd:
        local:
          extraArgs:
            metrics: extensive
            listen-metrics-urls: "http://0.0.0.0:9081"
      controllerManager:
        extraArgs:
          cloud-provider: external
    files:
      - content: |
          apiVersion: audit.k8s.io/v1
          kind: Policy
          # Don't generate audit events for all requests in the RequestReceived stage.
          omitStages:
            - "RequestReceived"
          rules:
            # Log events with metadata (requesting user, timestamp, resource, verb, etc.) but not request or
            # response body.
            - level: Metadata
              # Long-running requests like watches that fall under this rule will not generate an audit event
              # in RequestReceived stage. We will omit those logs.
              omitStages:
                - "RequestReceived"
        path: /srv/kubernetes/audit.yaml
    initConfiguration:
      skipPhases:
        - addon/kube-proxy
      nodeRegistration:
        kubeletExtraArgs:
          cloud-provider: external
        name: '{{ ds.meta_data.local_hostname }}'
    joinConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
          cloud-provider: external
        name: '{{ ds.meta_data.local_hostname }}'
    postKubeadmCommands:
      # Install Helm.
      - curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
      # Clone KubeAid.
      - git clone https://github.com/Obmondo/kubeaid.git
      # Copy kubeconfig to ~/.kube/config
      - mkdir -p $HOME/.kube
      - sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
      - sudo chown $(id -u):$(id -g) $HOME/.kube/config
      # Install Cilium.
      - kubectl create namespace cilium
      - API_SERVER_URL=$(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}')
      - API_SERVER_HOST=$(echo "$API_SERVER_URL" | awk -F[/:] '{print $4}')
      - API_SERVER_PORT=$(echo "$API_SERVER_URL" | awk -F[/:] '{print $5}')
      - helm template kubeaid/argocd-helm-charts/cilium --values kubeaid/argocd-helm-charts/cilium/values.yaml --set cilium.k8sServiceHost=${API_SERVER_HOST} --set cilium.k8sServicePort=${API_SERVER_PORT} --namespace cilium | kubectl apply -f -
      # Install AWS Cloud Controller Manager.
      - helm template kubeaid/argocd-helm-charts/ccm-aws --values kubeaid/argocd-helm-charts/ccm-aws/values.yaml --set hostNetworking=true --namespace kube-system | kubectl apply -f -
    users:
      - name: archi
        sshAuthorizedKeys:
          - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCMB0BJ1vL53/xiDCfswJWOrBYc39MbnmMnAK9ATohDqgT1ZrvLODbw5T1NTOLsVyuWNXMB9HfPPXg3Qm5L5UXoIj5JeyIY1ArUxjdW5LI4fsYgYHfRSv3gSZFpuJj12+myBxdoNYCDNDrzYF5T3D2is0lqcSG+3UerkY7ox8WuHMSO69bUAn72ugw01TgycdXpX5jvGLb84yl2Xct1/YFsh9UKhVYWx8qdz5r7FM4h5KDIIi2yjAEjwm4DfYKT/9dlzTodzeim4Z755AGXpFWjlQYaV8vNLHhIkDEtDOZTDtePFwQ64cw1e8cYzuRqDRxCVA9mWQtWwbMgklAv5h0ZTu8cJ9XcomQV9W2NCieBJyu+bj71AKFg3qTD9aMBq2zBXshoZwvmPlrIfAreALPEcPQvn6T8vHGvfk6Pf91B7I1Tu9ds0X0Vp5KELR6wwZQx2JKwMJillzC4Z3GY4/oi5J5C6WnlB3TxCvsivBo3TtEc3phylnfHNvRbwCqch2k= robot-webservice-app-user
  machineTemplate:
    infrastructureRef:
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
      kind: AWSMachineTemplate
      name: kubeaid-demo-aws-control-plane
  replicas: 3
  version: v1.31.0
---
# Source: capi-cluster/charts/aws/templates/MachineDeployment.yaml
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: kubeaid-demo-aws-primary-a
  annotations:
    cluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: "1"
    cluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: "1"
    capacity.cluster-autoscaler.kubernetes.io/memory: "4096G"
    capacity.cluster-autoscaler.kubernetes.io/cpu: "2"
    capacity.cluster-autoscaler.kubernetes.io/ephemeral-disk: "35Gi"
    capacity.cluster-autoscaler.kubernetes.io/maxPods: "500"
    capacity.cluster-autoscaler.kubernetes.io/labels: "node-role.kubernetes.io/bootstrapper=,node.cluster.x-k8s.io/nodegroup=bootstrapper"
    capacity.cluster-autoscaler.kubernetes.io/taints: ""
spec:
  clusterName: kubeaid-demo-aws
  replicas: 1
  template:
    metadata:
      # Label should meet one of the following criterias to propagate to Node :
      #
      # (1) Has node-role.kubernetes.io as prefix.
      # (2) Belongs to node-restriction.kubernetes.io domain.
      # (3) Belongs to node.cluster.x-k8s.io domain.
      #
      # REFER : https://cluster-api.sigs.k8s.io/developer/architecture/controllers/metadata-propagation#machine
      labels:
        node-role.kubernetes.io/bootstrapper: ""
        node.cluster.x-k8s.io/nodegroup: bootstrapper

    spec:
      clusterName: kubeaid-demo-aws
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: kubeaid-demo-aws-primary
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
        kind: AWSMachineTemplate
        name: kubeaid-demo-aws-primary
      failureDomain: "us-east-1a"
      version: v1.31.0
---
# Source: capi-cluster/charts/aws/templates/MachineDeployment.yaml
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: kubeaid-demo-aws-primary-b
  annotations:
    cluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: "0"
    cluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: "1"
    capacity.cluster-autoscaler.kubernetes.io/memory: "4096G"
    capacity.cluster-autoscaler.kubernetes.io/cpu: "2"
    capacity.cluster-autoscaler.kubernetes.io/ephemeral-disk: "35Gi"
    capacity.cluster-autoscaler.kubernetes.io/maxPods: "500"
    capacity.cluster-autoscaler.kubernetes.io/labels: "node-role.kubernetes.io/bootstrapper=,node.cluster.x-k8s.io/nodegroup=bootstrapper"
    capacity.cluster-autoscaler.kubernetes.io/taints: ""
spec:
  clusterName: kubeaid-demo-aws
  replicas: 0
  template:
    metadata:
      # Label should meet one of the following criterias to propagate to Node :
      #
      # (1) Has node-role.kubernetes.io as prefix.
      # (2) Belongs to node-restriction.kubernetes.io domain.
      # (3) Belongs to node.cluster.x-k8s.io domain.
      #
      # REFER : https://cluster-api.sigs.k8s.io/developer/architecture/controllers/metadata-propagation#machine
      labels:
        node-role.kubernetes.io/bootstrapper: ""
        node.cluster.x-k8s.io/nodegroup: bootstrapper

    spec:
      clusterName: kubeaid-demo-aws
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: kubeaid-demo-aws-primary
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
        kind: AWSMachineTemplate
        name: kubeaid-demo-aws-primary
      failureDomain: "us-east-1b"
      version: v1.31.0
---
# Source: capi-cluster/charts/aws/templates/MachineDeployment.yaml
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: kubeaid-demo-aws-primary-c
  annotations:
    cluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: "0"
    cluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: "1"
    capacity.cluster-autoscaler.kubernetes.io/memory: "4096G"
    capacity.cluster-autoscaler.kubernetes.io/cpu: "2"
    capacity.cluster-autoscaler.kubernetes.io/ephemeral-disk: "35Gi"
    capacity.cluster-autoscaler.kubernetes.io/maxPods: "500"
    capacity.cluster-autoscaler.kubernetes.io/labels: "node-role.kubernetes.io/bootstrapper=,node.cluster.x-k8s.io/nodegroup=bootstrapper"
    capacity.cluster-autoscaler.kubernetes.io/taints: ""
spec:
  clusterName: kubeaid-demo-aws
  replicas: 0
  template:
    metadata:
      # Label should meet one of the following criterias to propagate to Node :
      #
      # (1) Has node-role.kubernetes.io as prefix.
      # (2) Belongs to node-restriction.kubernetes.io domain.
      # (3) Belongs to node.cluster.x-k8s.io domain.
      #
      # REFER : https://cluster-api.sigs.k8s.io/developer/architecture/controllers/metadata-propagation#machine
      labels:
        node-role.kubernetes.io/bootstrapper: ""
        node.cluster.x-k8s.io/nodegroup: bootstrapper

    spec:
      clusterName: kubeaid-demo-aws
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: kubeaid-demo-aws-primary
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
        kind: AWSMachineTemplate
        name: kubeaid-demo-aws-primary
      failureDomain: "us-east-1c"
      version: v1.31.0
---
# Source: capi-cluster/charts/aws/templates/MachineDeployment.yaml
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: kubeaid-demo-aws-secondary-a
  annotations:
    cluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: "0"
    cluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: "1"
    capacity.cluster-autoscaler.kubernetes.io/memory: "4096G"
    capacity.cluster-autoscaler.kubernetes.io/cpu: "2"
    capacity.cluster-autoscaler.kubernetes.io/ephemeral-disk: "35Gi"
    capacity.cluster-autoscaler.kubernetes.io/maxPods: "500"
    capacity.cluster-autoscaler.kubernetes.io/labels: "node-role.kubernetes.io/scale-to-zero-test-subject=,node.cluster.x-k8s.io/nodegroup=scale-to-zero-test-subject"
    capacity.cluster-autoscaler.kubernetes.io/taints: ""
spec:
  clusterName: kubeaid-demo-aws
  replicas: 0
  template:
    metadata:
      # Label should meet one of the following criterias to propagate to Node :
      #
      # (1) Has node-role.kubernetes.io as prefix.
      # (2) Belongs to node-restriction.kubernetes.io domain.
      # (3) Belongs to node.cluster.x-k8s.io domain.
      #
      # REFER : https://cluster-api.sigs.k8s.io/developer/architecture/controllers/metadata-propagation#machine
      labels:
        node-role.kubernetes.io/scale-to-zero-test-subject: ""
        node.cluster.x-k8s.io/nodegroup: scale-to-zero-test-subject

    spec:
      clusterName: kubeaid-demo-aws
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: kubeaid-demo-aws-secondary
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
        kind: AWSMachineTemplate
        name: kubeaid-demo-aws-secondary
      failureDomain: "us-east-1a"
      version: v1.31.0
---
# Source: capi-cluster/charts/aws/templates/MachineDeployment.yaml
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: kubeaid-demo-aws-secondary-b
  annotations:
    cluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: "0"
    cluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: "0"
    capacity.cluster-autoscaler.kubernetes.io/memory: "4096G"
    capacity.cluster-autoscaler.kubernetes.io/cpu: "2"
    capacity.cluster-autoscaler.kubernetes.io/ephemeral-disk: "35Gi"
    capacity.cluster-autoscaler.kubernetes.io/maxPods: "500"
    capacity.cluster-autoscaler.kubernetes.io/labels: "node-role.kubernetes.io/scale-to-zero-test-subject=,node.cluster.x-k8s.io/nodegroup=scale-to-zero-test-subject"
    capacity.cluster-autoscaler.kubernetes.io/taints: ""
spec:
  clusterName: kubeaid-demo-aws
  replicas: 0
  template:
    metadata:
      # Label should meet one of the following criterias to propagate to Node :
      #
      # (1) Has node-role.kubernetes.io as prefix.
      # (2) Belongs to node-restriction.kubernetes.io domain.
      # (3) Belongs to node.cluster.x-k8s.io domain.
      #
      # REFER : https://cluster-api.sigs.k8s.io/developer/architecture/controllers/metadata-propagation#machine
      labels:
        node-role.kubernetes.io/scale-to-zero-test-subject: ""
        node.cluster.x-k8s.io/nodegroup: scale-to-zero-test-subject

    spec:
      clusterName: kubeaid-demo-aws
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: kubeaid-demo-aws-secondary
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
        kind: AWSMachineTemplate
        name: kubeaid-demo-aws-secondary
      failureDomain: "us-east-1b"
      version: v1.31.0
---
# Source: capi-cluster/charts/aws/templates/MachineDeployment.yaml
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: kubeaid-demo-aws-secondary-c
  annotations:
    cluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: "0"
    cluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: "0"
    capacity.cluster-autoscaler.kubernetes.io/memory: "4096G"
    capacity.cluster-autoscaler.kubernetes.io/cpu: "2"
    capacity.cluster-autoscaler.kubernetes.io/ephemeral-disk: "35Gi"
    capacity.cluster-autoscaler.kubernetes.io/maxPods: "500"
    capacity.cluster-autoscaler.kubernetes.io/labels: "node-role.kubernetes.io/scale-to-zero-test-subject=,node.cluster.x-k8s.io/nodegroup=scale-to-zero-test-subject"
    capacity.cluster-autoscaler.kubernetes.io/taints: ""
spec:
  clusterName: kubeaid-demo-aws
  replicas: 0
  template:
    metadata:
      # Label should meet one of the following criterias to propagate to Node :
      #
      # (1) Has node-role.kubernetes.io as prefix.
      # (2) Belongs to node-restriction.kubernetes.io domain.
      # (3) Belongs to node.cluster.x-k8s.io domain.
      #
      # REFER : https://cluster-api.sigs.k8s.io/developer/architecture/controllers/metadata-propagation#machine
      labels:
        node-role.kubernetes.io/scale-to-zero-test-subject: ""
        node.cluster.x-k8s.io/nodegroup: scale-to-zero-test-subject

    spec:
      clusterName: kubeaid-demo-aws
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: kubeaid-demo-aws-secondary
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
        kind: AWSMachineTemplate
        name: kubeaid-demo-aws-secondary
      failureDomain: "us-east-1c"
      version: v1.31.0
---
# Source: capi-cluster/charts/aws/templates/MachineHealthCheck.yaml
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineHealthCheck
metadata:
  name: kubeaid-demo-aws-kcp-unhealthy
spec:
  clusterName: kubeaid-demo-aws
  maxUnhealthy: 100%
  selector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: kubeaid-demo-aws
      cluster.x-k8s.io/control-plane: ""
  unhealthyConditions:
    - type: Ready
      status: Unknown
      timeout: 5m0s
    - type: Ready
      status: "False"
      timeout: 5m0s
